{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f307a60b",
   "metadata": {},
   "source": [
    "# Video Feature Extraction Tool - Demo Notebook\n",
    "\n",
    "This notebook demonstrates the **Video Feature Extraction Tool** that analyzes video files and extracts key visual and temporal features.\n",
    "\n",
    "## Features Implemented:\n",
    "1. **Shot Cut Detection** - Detects hard cuts in videos\n",
    "2. **Motion Analysis** - Quantifies motion using optical flow\n",
    "3. **Text Detection (OCR)** - Detects text presence in frames\n",
    "4. **Object vs Person Detection** - Analyzes object and person presence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ab781",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "First, let's install and import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e9c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_feature_extractor import VideoFeatureExtractor\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca7b7ed",
   "metadata": {},
   "source": [
    "## 2. Basic Usage - Quick Start\n",
    "\n",
    "Let's start with a simple example of how to use the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"sample_video.mp4\"\n",
    "\n",
    "extractor = VideoFeatureExtractor(video_path, sample_rate=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c2e4bd",
   "metadata": {},
   "source": [
    "## 3. Extract All Features at Once\n",
    "\n",
    "The easiest way to use the tool is to extract all features at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947df0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extractor.extract_all_features(\n",
    "    shot_cut_threshold=30.0,\n",
    "    text_sample_frames=20,\n",
    "    object_sample_frames=30,\n",
    "    object_confidence=0.5\n",
    ")\n",
    "\n",
    "extractor.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e61493",
   "metadata": {},
   "source": [
    "## 4. Individual Feature Extraction\n",
    "\n",
    "You can also extract features individually for more control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1493f6a",
   "metadata": {},
   "source": [
    "### 4.1 Shot Cut Detection\n",
    "\n",
    "Detects hard cuts (scene changes) in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a1566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shot_cuts = extractor.detect_shot_cuts(threshold=30.0)\n",
    "\n",
    "print(f\"Total cuts detected: {shot_cuts['total_cuts']}\")\n",
    "print(f\"Average frame difference: {shot_cuts['average_frame_difference']:.2f}\")\n",
    "\n",
    "if shot_cuts['cut_timestamps']:\n",
    "    print(\"\\nFirst 5 cuts:\")\n",
    "    for cut in shot_cuts['cut_timestamps'][:5]:\n",
    "        print(f\"  Frame {cut['frame']} at {cut['timestamp']:.2f}s (diff: {cut['difference']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffd12b",
   "metadata": {},
   "source": [
    "### 4.2 Motion Analysis\n",
    "\n",
    "Quantifies motion intensity using optical flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790141b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion = extractor.analyze_motion()\n",
    "\n",
    "print(f\"Motion Category: {motion['motion_category']}\")\n",
    "print(f\"Average Motion: {motion['average_motion']:.2f}\")\n",
    "print(f\"Max Motion: {motion['max_motion']:.2f}\")\n",
    "print(f\"Motion Std Dev: {motion['std_motion']:.2f}\")\n",
    "print(f\"Frames Analyzed: {motion['frames_analyzed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82240a41",
   "metadata": {},
   "source": [
    "### 4.3 Text Detection (OCR)\n",
    "\n",
    "Detects text presence in video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547951e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_detection = extractor.detect_text(sample_frames=20)\n",
    "\n",
    "if 'error' not in text_detection:\n",
    "    print(f\"Text Present Ratio: {text_detection['text_present_ratio']:.1%}\")\n",
    "    print(f\"Frames with Text: {text_detection['frames_with_text']}/{text_detection['frames_analyzed']}\")\n",
    "    \n",
    "    if text_detection['keywords']:\n",
    "        print(f\"\\nTop Keywords: {', '.join(text_detection['keywords'][:10])}\")\n",
    "    else:\n",
    "        print(\"\\nNo keywords detected\")\n",
    "else:\n",
    "    print(f\"Text detection unavailable: {text_detection['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ac265",
   "metadata": {},
   "source": [
    "### 4.4 Object and Person Detection\n",
    "\n",
    "Analyzes the presence of objects and people using YOLO or Haar Cascade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58260e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detection = extractor.detect_objects_and_people(confidence=0.5, sample_frames=30)\n",
    "\n",
    "if 'error' not in object_detection:\n",
    "    print(f\"People Detected: {object_detection['total_people_detected']}\")\n",
    "    \n",
    "    if 'total_objects_detected' in object_detection:\n",
    "        print(f\"Objects Detected: {object_detection['total_objects_detected']}\")\n",
    "        print(f\"Person/Object Ratio: {object_detection['person_to_object_ratio']:.2f}\")\n",
    "    \n",
    "    print(f\"Average People per Frame: {object_detection['average_people_per_frame']:.2f}\")\n",
    "    print(f\"Dominance: {object_detection['dominance'].capitalize()}\")\n",
    "    print(f\"Detection Method: {object_detection.get('detection_method', 'YOLO')}\")\n",
    "else:\n",
    "    print(f\"Object detection unavailable: {object_detection['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ae96ca",
   "metadata": {},
   "source": [
    "## 5. Visualizing Results\n",
    "\n",
    "Let's create some visualizations of the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f205fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "if 'shot_cuts' in features and features['shot_cuts']['cut_timestamps']:\n",
    "    ax = axes[0, 0]\n",
    "    cut_times = [cut['timestamp'] for cut in features['shot_cuts']['cut_timestamps']]\n",
    "    ax.eventplot(cut_times, colors='red', linewidths=2)\n",
    "    ax.set_xlabel('Time (seconds)')\n",
    "    ax.set_title(f\"Shot Cuts Timeline ({len(cut_times)} cuts)\")\n",
    "    ax.set_yticks([])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "if 'motion_analysis' in features:\n",
    "    ax = axes[0, 1]\n",
    "    motion_data = features['motion_analysis']\n",
    "    categories = ['Average\\nMotion', 'Max\\nMotion', 'Std\\nDev']\n",
    "    values = [motion_data['average_motion'], motion_data['max_motion'], motion_data['std_motion']]\n",
    "    bars = ax.bar(categories, values, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "    ax.set_ylabel('Motion Magnitude')\n",
    "    ax.set_title(f\"Motion Analysis ({motion_data['motion_category']})\")\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}', ha='center', va='bottom')\n",
    "\n",
    "if 'text_detection' in features and 'error' not in features['text_detection']:\n",
    "    ax = axes[1, 0]\n",
    "    text_data = features['text_detection']\n",
    "    ratio = text_data['text_present_ratio']\n",
    "    colors = ['#27ae60', '#ecf0f1']\n",
    "    sizes = [ratio, 1 - ratio]\n",
    "    labels = [f'Text Present\\n({ratio:.1%})', f'No Text\\n({1-ratio:.1%})']\n",
    "    ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    ax.set_title('Text Presence in Video')\n",
    "\n",
    "if 'object_detection' in features and 'error' not in features['object_detection']:\n",
    "    ax = axes[1, 1]\n",
    "    obj_data = features['object_detection']\n",
    "    \n",
    "    if 'total_objects_detected' in obj_data and obj_data['total_objects_detected'] > 0:\n",
    "        labels = ['People', 'Objects']\n",
    "        sizes = [obj_data['total_people_detected'], obj_data['total_objects_detected']]\n",
    "        colors = ['#3498db', '#e67e22']\n",
    "        ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "        ax.set_title(f\"Object Detection ({obj_data['dominance'].capitalize()} Dominant)\")\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f\"People Detected: {obj_data['total_people_detected']}\\n\\n\" +\n",
    "                f\"Avg per frame: {obj_data['average_people_per_frame']:.2f}\",\n",
    "                ha='center', va='center', fontsize=12, transform=ax.transAxes)\n",
    "        ax.set_title('Object Detection Results')\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dca5a0",
   "metadata": {},
   "source": [
    "## 6. Save Results to JSON\n",
    "\n",
    "Save the extracted features to a JSON file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c444ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.14.0' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "output_file = extractor.save_features()\n",
    "\n",
    "print(\"\\nExtracted Features (JSON format):\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(features, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773332f7",
   "metadata": {},
   "source": [
    "## 7. Advanced Usage: Batch Processing\n",
    "\n",
    "Process multiple videos at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b5b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def process_videos_in_folder(folder_path, output_folder=\"results\"):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    video_extensions = ['*.mp4', '*.avi', '*.mov', '*.mkv', '*.flv', '*.wmv']\n",
    "    \n",
    "    video_files = []\n",
    "    for ext in video_extensions:\n",
    "        video_files.extend(glob.glob(os.path.join(folder_path, ext)))\n",
    "    \n",
    "    print(f\"Found {len(video_files)} video files\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for i, video_path in enumerate(video_files, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing video {i}/{len(video_files)}: {os.path.basename(video_path)}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            extractor = VideoFeatureExtractor(video_path, sample_rate=30)\n",
    "            features = extractor.extract_all_features()\n",
    "            \n",
    "            base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "            output_path = os.path.join(output_folder, f\"{base_name}_features.json\")\n",
    "            extractor.save_features(output_path)\n",
    "            \n",
    "            results[video_path] = {\n",
    "                'success': True,\n",
    "                'output': output_path,\n",
    "                'features': features\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {video_path}: {e}\")\n",
    "            results[video_path] = {\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166b43b",
   "metadata": {},
   "source": [
    "## 8. Tips and Best Practices\n",
    "\n",
    "### Performance Optimization\n",
    "- **Sample Rate**: Increase `sample_rate` parameter to process fewer frames (faster but less accurate)\n",
    "- **Frame Selection**: For text detection, sample more frames if text is sparse\n",
    "- **Video Resolution**: Consider downscaling very high-resolution videos\n",
    "\n",
    "### Feature Tuning\n",
    "- **Shot Cut Threshold**: Lower values detect more cuts (30-40 is typical)\n",
    "- **OCR Accuracy**: Works best with clear, large text\n",
    "- **Object Detection**: Requires YOLO model files or uses Haar Cascade as fallback\n",
    "\n",
    "### Common Issues\n",
    "1. **Missing pytesseract**: Install with `pip install pytesseract` and Tesseract OCR\n",
    "2. **YOLO models not found**: Tool falls back to Haar Cascade for person detection\n",
    "3. **Memory issues**: Increase sample_rate or process shorter video segments\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This tool provides comprehensive video analysis with:\n",
    "- ✓ **Shot Cut Detection** - Identifies scene transitions\n",
    "- ✓ **Motion Analysis** - Quantifies camera and subject movement\n",
    "- ✓ **Text Detection** - Finds text in frames with OCR\n",
    "- ✓ **Object/Person Detection** - Analyzes visual content composition\n",
    "\n",
    "For questions or issues, refer to the README.md file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
